{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "#!pip install --upgrade git+https://github.com/EmGarr/kerod.git"
   ],
   "outputs": [],
   "metadata": {
    "id": "1jVKNjhdLFUQ"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "import  functools\n",
    "import  tensorflow as tf\n",
    "import  tensorflow_datasets as tfds\n",
    "from    tensorflow.keras.utils import to_categorical\n",
    "import  matplotlib.pyplot as plt\n",
    "from    tensorflow.keras import layers\n",
    "from    tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "import pandas as pd\n",
    "from    tqdm import tqdm\n",
    "import  time\n",
    "from    sklearn.metrics import accuracy_score\n",
    "import argparse\n",
    "\n",
    "from common.config import Config\n",
    "from common.logger import Logger\n",
    "from common.communication import Client\n",
    "from common.communication import Server\n",
    "from common.helper import ImagesInfo \n",
    "from common.timekeeper import TimeKeeper\n",
    "from common.helper import read_image, filt_text, get_predictions\n",
    "from CaptionModel import CaptionModel\n",
    "from common.helper import read_image, filt_text, get_predictions,process_predictions,get_reshape_size\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('-s', '--server', action='store', type=str, required=False)\n",
    "parser.add_argument('-t', '--test_number', action='store', type=int, required=False)\n",
    "parser.add_argument('-l', '--split_layer', action='store', type=int, required=False)\n",
    "parser.add_argument('-v', '--verbose', action='store', type=int, required=False)\n",
    "parser.add_argument('-i', '--image_size', action='store', type=int, required=False)\n",
    "parser.add_argument('-m', '--max_tests', action='store', type=int, required=False)\n",
    "args, unknown = parser.parse_known_args()\n",
    "\n",
    "split_layer = args.split_layer\n",
    "\n",
    "if(split_layer == None):\n",
    "    split_layer = 3\n",
    "\n",
    "Logger.milestone_print(\"Splitting at layer %d\" % split_layer)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "data_dir='/home/suphale/coco'\n",
    "split_train = \"train[:1%]\"\n",
    "split_val = \"validation[:1%]\"\n",
    "image_size = 250\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "tk = TimeKeeper()\n",
    "cfg = Config()\n",
    "client = Client(cfg)\n",
    "imagesInfo = ImagesInfo(cfg)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "from tensorflow import keras  # or import keras for standalone version\n",
    "from tensorflow.keras.layers import Input\n",
    "\n",
    "org_model = tf.keras.models.load_model(cfg.saved_model_path + '/iv3_full_model', compile=False)\n",
    "\n",
    "# basic_model = tf.keras.models.load_model(cfg.saved_model_path + '/model')\n",
    "# org_model = tf.keras.Model(inputs=basic_model.input, \n",
    "#         outputs=[basic_model.get_layer('mixed10').output, basic_model.get_layer('dense_1').output] )"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "model_config = org_model.get_config()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "max_layer_index = len(model_config['layers']) - 1"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "\n",
    "head_model_config = {}\n",
    "head_model_config['name'] = 'head_model'\n",
    "head_model_config['layers'] = []\n",
    "head_model_config['input_layers'] = [[model_config['layers'][0]['name'],0,0]]\n",
    "head_model_config['output_layers'] = [[model_config['layers'][split_layer+1-1]['name'],0,0]]\n",
    "\n",
    "for index in range(split_layer+1):\n",
    "    head_model_config['layers'].append(model_config['layers'][index])\n",
    "\n",
    "print(\"Final layer of head model [%d] %s\" % (split_layer, model_config['layers'][split_layer]['name']) )\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Final layer of head model [3] activation\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "# Last layer of the head model\n",
    "last_head_model_layer = head_model_config['layers'][split_layer]['name']\n",
    "# print(last_head_model_layer)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "# First layer of the tail model\n",
    "print(\"First layer of tail model [%d] %s\" % (split_layer+1, model_config['layers'][split_layer+1]['name']) )\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "First layer of tail model [4] conv2d_1\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "import copy\n",
    "\n",
    "tail_model_config = copy.deepcopy(model_config)\n",
    "tail_model_config['name'] = 'tail_model'\n",
    "tail_model_config['input_layers'] = [[model_config['layers'][split_layer+1]['name'],0,0]]\n",
    "# tail_model_config['output_layers'] = [[model_config['layers'][max_layer_index]['name'],0,0]]\n",
    "tail_model_config['output_layers'] = model_config['output_layers']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "# print(tail_model_config['input_layers'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "# print(tail_model_config['output_layers'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "new_input_layer = {\n",
    "                      'name': 'new_input',\n",
    "                      'class_name': 'InputLayer',\n",
    "                      'config': {\n",
    "                          'batch_input_shape': tuple(org_model.layers[split_layer+1-1].output.shape),\n",
    "                          'dtype': 'float32',\n",
    "                          'sparse': False,\n",
    "                          'name': 'new_input'\n",
    "                      },\n",
    "                      'inbound_nodes': []\n",
    "                  }\n",
    "tail_model_config['layers'][0] = new_input_layer"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "for index in range(1,split_layer+1):\n",
    "    # print(\"%d %s\" % (index, tail_model_config['layers'][1]['name']) )\n",
    "    tail_model_config['layers'].pop(1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "import numpy as np\n",
    "\n",
    "# Find if any layer in the tail model takes the last layer of head model as input\n",
    "# substitute it with the input layer\n",
    "# Ideally we should check if any tail model layer refers to any head model layer ToDo\n",
    "for index, layer in enumerate(tail_model_config['layers']):\n",
    "    if (np.shape(layer['inbound_nodes'])[0] > 0):\n",
    "        dim_1 = len(layer['inbound_nodes'][0])\n",
    "        if(dim_1 >= 1):\n",
    "            for i in range(dim_1):\n",
    "                in_layer = layer['inbound_nodes'][0][i][0]\n",
    "                if(in_layer == last_head_model_layer):\n",
    "                    print(str(index) + \"    \" + layer['name'] + \" -> \" + in_layer )\n",
    "                    # print(tail_model_config['layers'][index]['inbound_nodes'][0][i])\n",
    "                    tail_model_config['layers'][index]['inbound_nodes'][0][i] = [[['new_input', 0, 0, {}]]]\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1    conv2d_1 -> activation\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "# tail_model_config['layers'][1]['inbound_nodes'] = [[['new_input', 0, 0, {}]]]\n",
    "tail_model_config['input_layers'] = [['new_input', 0, 0]]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "import pprint\n",
    "with open(cfg.temp_path + '/model_config.txt','w') as fh:\n",
    "    # Pass the file handle in as a lambda function to make it callable\n",
    "    # fh.write(str(model_config))\n",
    "    print(model_config,file=fh)\n",
    "with open(cfg.temp_path + '/head_model_config.txt','w') as fh:\n",
    "    # Pass the file handle in as a lambda function to make it callable\n",
    "    # fh.write(str(new_head_model_config))\n",
    "    print(head_model_config,file=fh)\n",
    "with open(cfg.temp_path + '/tail_model_config.txt','w') as fh:\n",
    "    # Pass the file handle in as a lambda function to make it callable\n",
    "    # fh.write(str(new_head_model_config))\n",
    "    print(tail_model_config,file=fh)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "head_model = org_model.__class__.from_config(head_model_config, custom_objects={})"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "tail_model = org_model.__class__.from_config(tail_model_config, custom_objects={})"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "with open(cfg.temp_path + '/org_model.txt','w') as fh:\n",
    "    org_model.summary(print_fn=lambda x: fh.write(x + '\\n'), line_length=150)\n",
    "\n",
    "with open(cfg.temp_path + '/head_model.txt','w') as fh:\n",
    "    head_model.summary(print_fn=lambda x: fh.write(x + '\\n'), line_length=150)\n",
    "\n",
    "with open(cfg.temp_path + '/tail_model.txt','w') as fh:\n",
    "    tail_model.summary(print_fn=lambda x: fh.write(x + '\\n'), line_length=150)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "for index, layer in enumerate(org_model.layers[:split_layer+1]):\n",
    "    # print(\"[%d] %s %s\" % (index, layer.name, str(np.shape(weight))))\n",
    "    weight = layer.get_weights()\n",
    "    new_head_model_layer = head_model.layers[index]\n",
    "    new_head_model_layer.set_weights(weight)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "import numpy as np\n",
    "for index, layer in enumerate(org_model.layers[split_layer+1:max_layer_index+1]):\n",
    "    weight = layer.get_weights()\n",
    "    # print(\"org_model [%d] %s %s\" % (index, layer.name, str(tf.shape(weight))))\n",
    "    tail_model_layer = tail_model.layers[index+1]\n",
    "    tail_model_layer_weight = tail_model_layer.get_weights()\n",
    "    # print(\"tail_model [%d] %s %s\" % (index, tail_model_layer.name, str(tf.shape(tail_model_layer_weight))))\n",
    "    tail_model_layer.set_weights(weight)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "head_model.save(cfg.temp_path + '/iv3_head_model_'+str(split_layer))\n",
    "tail_model.save(cfg.temp_path + '/iv3_tail_model_'+str(split_layer))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "INFO:tensorflow:Assets written to: /home/suphale/WorkSpace/temp/iv3_head_model_3/assets\n",
      "INFO:tensorflow:Assets written to: /home/suphale/WorkSpace/temp/iv3_tail_model_3/assets\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "class BoxField:\n",
    "    BOXES = 'bbox'\n",
    "    KEYPOINTS = 'keypoints'\n",
    "    LABELS = 'label'\n",
    "    MASKS = 'masks'\n",
    "    NUM_BOXES = 'num_boxes'\n",
    "    SCORES = 'scores'\n",
    "    WEIGHTS = 'weights'\n",
    "\n",
    "class DatasetField:\n",
    "    IMAGES = 'images'\n",
    "    IMAGES_INFO = 'images_information'\n",
    "    IMAGES_PMASK = 'images_padding_mask'\n",
    "\n",
    "def my_preprocess(inputs):\n",
    "    image = inputs['image']\n",
    "    image = tf.image.resize(image, (image_size, image_size))\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image /= 127.5\n",
    "    image -= 1.\n",
    "\n",
    "    targets = inputs['objects']\n",
    "    img_path = inputs['image/filename']\n",
    "\n",
    "    image_information = tf.cast(tf.shape(image)[:2], dtype=tf.float32)\n",
    "\n",
    "    inputs = {DatasetField.IMAGES: image, DatasetField.IMAGES_INFO: image_information}\n",
    "\n",
    "    # ground_truths = {\n",
    "    #     BoxField.BOXES: targets[BoxField.BOXES] * tf.tile(image_information[tf.newaxis], [1, 2]),\n",
    "    #     BoxField.LABELS: tf.cast(targets[BoxField.LABELS], tf.int32),\n",
    "    #     BoxField.NUM_BOXES: tf.shape(targets[BoxField.LABELS]),\n",
    "    #     BoxField.WEIGHTS: tf.fill(tf.shape(targets[BoxField.LABELS]), 1.0)\n",
    "    # }\n",
    "    ground_truths = tf.cast(targets[BoxField.LABELS], tf.int32)\n",
    "    # ground_truths = tf.one_hot(ground_truths, depth=N_LABELS, dtype=tf.int32)\n",
    "    # ground_truths = tf.reduce_sum(ground_truths, 0)\n",
    "    # ground_truths = tf.greater( ground_truths, tf.constant( 0 ) )    \n",
    "    # ground_truths = tf.where (ground_truths, 1, 0) \n",
    "    return image, ground_truths, img_path\n",
    "\n",
    "def expand_dims_for_single_batch(image, ground_truths, img_path):\n",
    "    image = tf.expand_dims(image, axis=0)\n",
    "    ground_truths = tf.expand_dims(ground_truths, axis=0)\n",
    "    return image, ground_truths, img_path"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "ds_val, ds_info = tfds.load(name=\"coco/2017\", split=split_val, data_dir=data_dir, shuffle_files=False, download=False, with_info=True)\n",
    "ds_val = ds_val.map(functools.partial(my_preprocess), num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "ds_val = ds_val.map(expand_dims_for_single_batch, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "ds_val = ds_val.prefetch(tf.data.experimental.AUTOTUNE)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function my_preprocess at 0x7f8b68391830> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function my_preprocess at 0x7f8b68391830> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "WARNING: AutoGraph could not transform <function my_preprocess at 0x7f8b68391830> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "from   nltk.translate.bleu_score import sentence_bleu\n",
    "def process_caption_predictions(caption_tensor, img_path):\n",
    "    pred_caption=' '.join(caption_tensor).rsplit(' ', 1)[0]\n",
    "    real_appn = []\n",
    "    real_caption_list = imagesInfo.annotations_dict[img_path]\n",
    "    for real_caption in real_caption_list:\n",
    "        real_caption=filt_text(real_caption)\n",
    "        real_appn.append(real_caption.split())\n",
    "    reference = real_appn\n",
    "    candidate = pred_caption.split()\n",
    "    score = sentence_bleu(reference, candidate, weights=[1]) #set your weights)\n",
    "    return score,real_caption,pred_caption"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "Test = False\n",
    "if (Test == True):\n",
    "    captionModel = CaptionModel()\n",
    "    real_caption_list = []\n",
    "    pred_caption_list = []\n",
    "    max_test_count = 10\n",
    "    ds_val = ds_val.take(max_test_count)\n",
    "    for sample_img_batch, ground_truth, img_path in tqdm(ds_val):\n",
    "        # count += 1\n",
    "\n",
    "        tensor_shape = len(ground_truth.get_shape().as_list())\n",
    "        if(tensor_shape > 1):\n",
    "            ground_truth = tf.squeeze(ground_truth,[0])\n",
    "        ground_truth = list(set(ground_truth.numpy()))\n",
    "\n",
    "        img_path = img_path.numpy().decode()\n",
    "        h = head_model(sample_img_batch)\n",
    "        features, result = tail_model(h)\n",
    "        # features, result = model(sample_img_batch)\n",
    "        predictions, predictions_prob = get_predictions(cfg, result)\n",
    "        accuracy, top_1_accuracy,top_5_accuracy,precision,recall, top_predictions, predictions_str = process_predictions(cfg, imagesInfo, ground_truth,predictions, predictions_prob)\n",
    "\n",
    "        reshape_layer_size = get_reshape_size(image_size)\n",
    "        features = tf.reshape(features, [sample_img_batch.shape[0],reshape_layer_size*reshape_layer_size, 2048])\n",
    "        caption_tensor = captionModel.evaluate(features)\n",
    "\n",
    "        score,real_caption,pred_caption = process_caption_predictions(caption_tensor, img_path)\n",
    "\n",
    "        real_caption_list.append(real_caption)\n",
    "        pred_caption_list.append(pred_caption)\n",
    "\n",
    "    for i in range(max_test_count):\n",
    "        print ('Real:', real_caption_list[i])\n",
    "        print ('Pred:', pred_caption_list[i])    "
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 10/10 [00:08<00:00,  1.20it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Real: a double decker bus is stopped along a curb\n",
      "Pred: a bus is driving down the street\n",
      "Real: a man playing tennis on a red coat in all white\n",
      "Pred: a young man playing a baseball\n",
      "Real: this video game has a wii device and instructions\n",
      "Pred: a   p a i r   o f   s c i s s o r s   a n d   a   p a i r   o f   s c i s s o r s   a n d   a   p a i r   o f   s c i s s o r s   a n d   a   p a i r   o f   s c i s s o r s   a n d   a   p a i r   o f   s c i s s o r\n",
      "Real: a man holds a sign advertising a deli\n",
      "Pred: a man on a street\n",
      "Real: a kitchen with hardwood floors and a sink and oven\n",
      "Pred: a large white and white and a large mirror and a large mirror\n",
      "Real: a stop sign on the side of a street\n",
      "Pred: a stop sign on a street\n",
      "Real: these two elephants look like they are fighting\n",
      "Pred: a baby elephant is standing next to a baby elephant\n",
      "Real: there is a woman sitting outside writing and drinking coffee\n",
      "Pred: a woman sitting on a cell phone\n",
      "Real: a view of a bunch of pizzas sitting on a table\n",
      "Pred: a   p l a t e   w i t h   a   p l a t e   w i t h   a   p l a t e   w i t h   a   p l a t e   w i t h   a   p l a t e   w i t h   a   p l a t e   w i t h   a   p l a t e   w i t h   a   p l a t e   w i t\n",
      "Real: several mounted police officers and their horses line up on the street\n",
      "Pred: a group of people on a race\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "Test = True\n",
    "if (Test == True):\n",
    "    captionModel = CaptionModel(image_size=image_size)\n",
    "    count = 0\n",
    "    ds_val = ds_val.take(2)\n",
    "    for sample_img_batch, ground_truth, img_path in ds_val:\n",
    "        count += 1\n",
    "\n",
    "        tensor_shape = len(ground_truth.get_shape().as_list())\n",
    "        if(tensor_shape > 1):\n",
    "            ground_truth = tf.squeeze(ground_truth,[0])\n",
    "        ground_truth = list(set(ground_truth.numpy()))\n",
    "\n",
    "        img_path = img_path.numpy().decode()\n",
    "        print(img_path)\n",
    "        # features, result = org_model(sample_img_batch)\n",
    "        h = head_model(sample_img_batch)\n",
    "        features, result = tail_model(h)\n",
    "\n",
    "        predictions, predictions_prob = get_predictions(cfg, result)\n",
    "        accuracy, top_1_accuracy,top_5_accuracy,precision,recall, top_predictions, predictions_str = process_predictions(cfg, imagesInfo, ground_truth,predictions, predictions_prob)\n",
    "        # print(predictions_str)\n",
    "\n",
    "        features = tf.reshape(features, [sample_img_batch.shape[0],get_reshape_size(image_size)*get_reshape_size(image_size), 2048])\n",
    "        caption_tensor = captionModel.evaluate(features)\n",
    "\n",
    "        # print(type(caption_tensor))\n",
    "        score,real_caption,pred_caption = process_caption_predictions(caption_tensor, img_path)\n",
    "\n",
    "        print(\"BLEU: %.2f\" % (score))\n",
    "        print ('Real:', real_caption)\n",
    "        print ('Pred:', pred_caption)    "
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "000000531036.jpg\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      " 50%|█████     | 1/2 [00:03<00:03,  3.51s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "person(59.00) car(49.00) bus(92.00) truck(44.00) \n",
      "<class 'list'>\n",
      "BLEU: 0.43\n",
      "Real: a double decker bus is stopped along a curb\n",
      "Pred: a bus is driving down the street\n",
      "000000133418.jpg\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 2/2 [00:04<00:00,  2.06s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "person(99.00) sports ball(58.00) tennis racket(99.00) \n",
      "<class 'list'>\n",
      "BLEU: 0.48\n",
      "Real: a man playing tennis on a red coat in all white\n",
      "Pred: a young man playing a baseball\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 2/2 [00:04<00:00,  2.29s/it]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "if (False):\n",
    "    for index, layer in enumerate(model_config['layers']):\n",
    "        if (np.shape(layer['inbound_nodes'])[0] > 0):\n",
    "            dim_1 = len(layer['inbound_nodes'][0])\n",
    "            if(dim_1 > 1):\n",
    "                print(index, layer['name'])\n",
    "                for i in range(dim_1):\n",
    "                    print(\"    %s\" % (layer['inbound_nodes'][0][i][0]))\n",
    "                    # print(tail_model_config['layers'][index]['inbound_nodes'][0][i])\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# org_model.save(cfg.temp_path + '/full_model')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "coco_training.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.10 64-bit ('py373': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "interpreter": {
   "hash": "53d8a323e6010706682c07af791323eacfc072764aa514c33420848fded080be"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}